{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "- [TensorFlow Example - convolution network](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3%20-%20Neural%20Networks/convolutional_network.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_HOME'] = '/usr/local/cuda'\n",
    "os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda/lib64/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./train-images-idx3-ubyte.gz\n",
      "Extracting ./train-labels-idx1-ubyte.gz\n",
      "Extracting ./t10k-images-idx3-ubyte.gz\n",
      "Extracting ./t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/gzip.py:268: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n",
      "  chunk = self.extrabuf[offset: offset + size]\n",
      "input_data.py:41: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n",
      "  data = data.reshape(num_images, rows, cols, 1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"./\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "num_iters = 1000\n",
    "batch_size = 64\n",
    "display_step = 20\n",
    "\n",
    "geometry = [28, 28]\n",
    "classes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "num_classes = len(classes)\n",
    "dropout_prob = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tensor Flow Graph Input\n",
    "X = tf.placeholder(tf.float32, [None, geometry[0]*geometry[1]])\n",
    "y = tf.placeholder(tf.float32, [None, num_classes])\n",
    "dropout = tf.placeholder(tf.float32)\n",
    "\n",
    "# AlexNet Weight & bias\n",
    "# 3x3 conv, 1 input, 64 outputs\n",
    "Wc1 = tf.Variable(tf.random_normal([3, 3, 1, 64]))\n",
    "bc1 = tf.Variable(tf.random_normal([64]))\n",
    "\n",
    "# 3x3 conv, 64 input, 128 outputs\n",
    "Wc2 = tf.Variable(tf.random_normal([3, 3, 64, 128]))\n",
    "bc2 = tf.Variable(tf.random_normal([128]))\n",
    "\n",
    "# 3x3 conv, 128 input, 256 outputs\n",
    "Wc3 = tf.Variable(tf.random_normal([3, 3, 128, 256]))\n",
    "bc3 = tf.Variable(tf.random_normal([256]))\n",
    "\n",
    "# Fully connected (Standard 3-layer MLP), 4*4*256 input, 1024 \n",
    "Wf1 = tf.Variable(tf.random_normal([4*4*256, 1024]))\n",
    "bf1 = tf.Variable(tf.random_normal([1024]))\n",
    "\n",
    "Wf2 = tf.Variable(tf.random_normal([1024, 1024]))\n",
    "bf2 = tf.Variable(tf.random_normal([1024]))\n",
    "\n",
    "Wout = tf.Variable(tf.random_normal([1024, num_classes]))\n",
    "bout = tf.Variable(tf.random_normal([num_classes]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convolution Network\n",
    "\n",
    "# Reshape input picture\n",
    "input_X = tf.reshape(X, shape=[-1, 28, 28, 1])\n",
    "\n",
    "# Stage 1 : Convolution -> ReLU -> Max Pooling -> Local Response Normalization -> Dropout\n",
    "conv1 = tf.nn.conv2d(input_X, Wc1, strides = [1, 1, 1, 1], padding='SAME')\n",
    "conv1 = tf.nn.relu(tf.nn.bias_add(conv1, bc1))\n",
    "conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides = [1, 2, 2, 1], padding='SAME')\n",
    "conv1 = tf.nn.lrn(conv1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm1')\n",
    "conv1 = tf.nn.dropout(conv1, dropout)\n",
    "\n",
    "# Stage 2 : Convolution -> ReLU -> Max Pooling -> Local Response Normalization -> Dropout\n",
    "conv2 = tf.nn.conv2d(conv1, Wc2, strides = [1, 1, 1, 1], padding='SAME')\n",
    "conv2 = tf.nn.relu(tf.nn.bias_add(conv2, bc2))\n",
    "conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides = [1, 2, 2, 1], padding='SAME')\n",
    "conv2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm2')\n",
    "conv2 = tf.nn.dropout(conv2, dropout)\n",
    "\n",
    "# Stage 3 : Convolution -> ReLU -> Max Pooling -> Local Response Normalization -> Dropout\n",
    "conv3 = tf.nn.conv2d(conv2, Wc3, strides = [1, 1, 1, 1], padding='SAME')\n",
    "conv3 = tf.nn.relu(tf.nn.bias_add(conv3, bc3))\n",
    "conv3 = tf.nn.max_pool(conv3, ksize=[1, 2, 2, 1], strides = [1, 2, 2, 1], padding='SAME')\n",
    "conv3 = tf.nn.lrn(conv3, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm3')\n",
    "conv3 = tf.nn.dropout(conv3, dropout)\n",
    "\n",
    "# Stage 4 : Fully connected : Linear -> ReLU -> Linear\n",
    "fc1 = tf.reshape(conv3, [-1, Wf1.get_shape().as_list()[0]])\n",
    "fc1 = tf.nn.relu(tf.add(tf.matmul(fc1, Wf1), bf1))\n",
    "fc2 = tf.nn.relu(tf.add(tf.matmul(fc1, Wf2), bf2))\n",
    "\n",
    "out = tf.add(tf.matmul(fc2, Wout), bout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(out, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(out, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Launch the Graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Train\n",
    "    for epoch in range(1, num_iters+1):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        # Fit training data\n",
    "        \n",
    "        sess.run(optimizer, feed_dict={X: batch_xs, y: batch_ys, dropout: dropout_prob})\n",
    "        \n",
    "        if epoch & 50 == 0:\n",
    "            loss = sess.run(cost, feed_dict={X: batch_xs, y: batch_ys, dropout: 1.})\n",
    "            print(\"Epoch : \", epoch, \" loss=\" , loss)\n",
    "    \n",
    "    print(\"Optimization Finishied\")\n",
    "    \n",
    "    # Test\n",
    "    print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={X: mnist.test.images, \n",
    "                                                             y: mnist.test.labels, \n",
    "                                                             dropout: 1.}) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
